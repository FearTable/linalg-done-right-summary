\section{Invariant Subspaces}
\subsection{Eigenvalues}

\begin{mydef}
  A \lm from a \vs to itself is called an ``operator".
\end{mydef}

(Suppose $T\in \linmap(V)$, then may be $\left.T\right|_{V_{k}}$ is not an operator on a subspace $V_k$)

\begin{mydef}
  Let $T\in \linmap(V).$ $U \subseteq V$ is called ``invariant under $T$" if $\forall u \in U: Tu \in U.$ \\
  Thus $U$ is invariant under $T$ if $\left.T\right|_{U}$ is an operator on $U.$
\end{mydef}

\begin{example}
  Let $T\in \linmap(\mathcal{P}(\mathbb{R}))$ such that $Tp=p'.$ Let $U=\mathcal{P}_4(\mathbb{R}) \subseteq \mathcal{P}(\mathbb{R}).$ Then $U$ is invariant under $T$
  because if $p \in U$, $\deg p = 4$ and $\deg (p')=3$.
\end{example}

\begin{example}
  Let $T\in \linmap(V)$. Then $\{0\}, V, \operatorname{null} T, \operatorname{range} T$ are all invariant. \\
  (Sometimes, $\operatorname{null} T = \{0\}$ and $\operatorname{range} T=V$ if $T$ is invertible.)
\end{example}

\bigbreak

\bfemph{Invariant subspaces of dimension one:} \\
Take any $v\in V, v\neq 0$ and let $U :\equiv \{  \lambda v \mid \lambda \in \myF \} = \myspan{v}$, then $U$ is a one-dimensional subspace of $V$. \\
If $U$ is invariant under an operator $T \in \linmap (V)$, then $Tv  \in U$. $\implies \exists \lambda \in \myF: Tv = \lambda v$. \\
Conversely if $Tv = \lambda v$, $\lambda \in \myF$, then $\myspan{v}$ is a one-dimensional subspace of $V$ invariant under $T$.

\begin{mydef}
  Suppose $T\in \linmap (V)$. $\lambda \in \myF$ is called ``eigenvalue of $T$" if there exists $v \in V$ such that $v \neq 0$ and $Tv = \lambda v$
\end{mydef}

\setcounter{thm}{6}
\begin{thm}
  \label{equivalent-conditions-to-be-an-eigenvalue}
  The following are equivalent for $T \in \linmap(V)$ and $\lambda \in \myF$:
  \begin{enumerate}[label=(\alph*)]
    \item $\lambda$ is an eigenvalue of $T$. \label{first}
    \item $T-\lambda I$ is not injective. \label{second}
    \item $T-\lambda I$ is not surjective. \label{third}
    \item $T-\lambda I$ is not invertible. \label{forth}
  \end{enumerate}
\end{thm}
\begin{prf}
  Conditions \ref{first} and \ref{second} are equivalent because the eigenvector $v$ is a solution to 
  \begin{equation}
    Tv=\lambda v
  \end{equation} which is equivalent to 
  \begin{equation}
    (T-\lambda I)v=0.
  \end{equation} So there is a non-zero solution to $T-\lambda I$.
  \ref{second}, \ref{third} and \ref{forth} are equivalent by \ref{injectivity-is-equivalent-to-surjectivity}.
  
\end{prf}

\setcounter{thm}{7}
\begin{mydef}
  Let $T\in \linmap(V).$ A vector $v \in V$ is called ``an eigenvector" of $T$ corresponding to $\lambda$ if $v\neq 0$ and $Tv = \lambda v$.
  In other words:
  \\A vector $v\in V, v \neq 0$ is an eigenvector corresponding to $\lambda \iff v \in \operatorname{null}(T-\lambda I_V)$
\end{mydef}

\setcounter{thm}{10}
\begin{thm}
  Every list of eigenvectors of $T$ corresponding to distinct eigenvalues of $T$ is linearly independent.
\end{thm}
\begin{prf}
  Suppose the desired result is false. Then there exists a smallest list of length $m$ of linearly dependent eigenvectors $v_1, \dots, v_m$ with eigenvalues $\lambda_1, \dots, \lambda_m$ of $T$. Since an eigenvector is unequal to the zero vector, $m$ must be $\geq 2$.

  Because of the minimality of $m$ and becaue our list is linearly dependent: $\exists a_1, \dots, a_m \neq 0$ such that $a_1 v_1 + \cdots + a_m v_m = 0$. 
  
  Now we apply $T-\lambda_m I$ on both sides of the equation and get
  \begin{equation}
    \begin{gathered}
      a_1 \lambda_1 v_1 - a_1 \lambda_m v_1
      + \cdots + \\
      a_{m-1} \lambda_{m-1} v_{m-1} - a_{m-1} \lambda_{m} v_{m-1} +
      \underbrace{a_m \lambda_m v_m -a_m \lambda_m v_m}_{=0} =0
    \end{gathered}
  \end{equation}

  Which is the same as:
  \begin{equation}
    a_1 \underbrace{(\lambda_1 - \lambda_m)}_{\neq 0} v_1 + \cdots + a_{m-1} \underbrace{(\lambda_{m-1}-\lambda_{m})}_{\neq 0} v_{m-1}=0
  \end{equation}

  Which contradicts the minimality of $m$. Therefore, no such linearly dependent list of eigenvectors can exist.
  
\end{prf}

\begin{thm}
  Each operator on $V$ has at most $\dim V$ distinct eigenvalues.
  content
\end{thm}

\paragraph{}

\subsection{Polynomials applied to operators}

\setcounter{thm}{12}
\begin{mydef}
  Let $T \in \linmap(V)$ and $m\in \nat^{+}$
  \begin{itemize}
    \item $T^{m} :\equiv \underbrace{T \cdots T}_{\text{$m$ times}}$ or $T^{m} :\equiv T^{m-1} \cdot T$ such that $T^{m} \in \linmap(V)$
    \item $T^0 :\equiv I_V$
    \item If $T$ is invertible with inverse $T^{-1}$ then $T^{-m}\in \linmap(V)$ is defined by $T^{-m} :\equiv (T^{-1})^m$
  \end{itemize}
\end{mydef}
$\implies T^m T^n = T^{m+n}$ and $(T^m)^n=T^{mn}$ when $m,n \in \mathbb{Z}$ when $T$ is invertible. And $m,n \in \mathbb{N}$ if $T$ is not invertible.

\begin{mydef}
  For $p \in \mathcal{P} (\myF)$, $p(z) = a_0+a_1z+a_2z^2+\cdots+a_mz^m$
  $\forall z \in \compl$ and
  $T \in \linmap (V)$ we define: \\
  \begin{equation}
    p(T) :\equiv a_0 I + a_1 T + a_2 T^2 + \cdots a_m T^m,$ $p(T) \in \linmap(V)
  \end{equation}
\end{mydef}

%TODO: example 5.15??

%TODO: example 5.16??

\setcounter{thm}{16}
\begin{thm}
  \label{multiplicative-properties}
  Suppose $p,q \in \mathcal{P} (\myF)$ and $T\in \linmap (V)$. Then \begin{equation}
    (p q)(T) = p(T) q(T) = q(T)p(T).
  \end{equation}
\end{thm}

\begin{thm}
  \label{null-space-and-range-of-p(T)-are-invariant-under-T}
  $T \in \linmap(V)$ and $p\in \mathcal{P} (\myF) \implies$
  $\operatorname{null} p(T)$ and $\operatorname{range} p(T)$ are invariant under $T$.
\end{thm}
\begin{prf}
  Suppose $u\in \operatorname{null} p(T) \implies p(T)u = 0$. Assoziativiy and distributivity of linear maps imply that 
  \begin{equation}
    (p(T))(Tu)=T(p(T)u)=T(0)=0.
  \end{equation}
  
  Which implies that
  \begin{equation}
    Tu \in \mynull p(T).
  \end{equation}

  Suppose $u \in \myrange p(T)$
  \begin{equation}
    \begin{aligned}
    &\implies \exists v\in V: u=p(T)v \\
    &\implies Tu=T(p(T)v)=p(T)(Tv) \\
    &\implies Tu \in \myrange p(T)
    \end{aligned}
  \end{equation}
\end{prf}

\section{The Minimal Polynomial}
\subsection{Existence of Eigenvalues on Complex Vector Spaces}

\begin{thm}
  Every operator on a finite-dimensional nonzero complex vector space has an eigenvalue.
\end{thm}
\begin{prf}
  Suppose $\dim(V)=n>0$ and $T\in \linmap(V).$ 
  
  Choose $v\in V, v\neq0$. 
  
  Then 
  \begin{equation}
    v, Tv, T^2v, \dots, T^nv
  \end{equation} 
  is not linearly independent, 
  because the list has length $n+1$. Therefore, some linear combination of these vectors equals to $0$. 
  
  $\implies$ there exists a non-constant polynomial $p$ of smallest degree such that $p(T)v = 0$. By the first version of the fundamental theorem of algebra (\ref{fundamental-theorem-of-algebra-first-version}) 
  \begin{equation}
    \implies \exists \lambda \in \compl: p(\lambda) = 0.
  \end{equation}
  
  (\ref{factororing-out-zeros-of-a-polynomial-always-possible})
  \begin{equation}
    \implies \exists q \in \mathcal {P} (\compl): p(z) = (z-\lambda)  q(z) \; \forall z \in \compl 
  \end{equation}
  
  (\ref{multiplicative-properties})
  \begin{equation}
    \implies 0=p(T)v=(T-\lambda I) (q(T)v). 
  \end{equation}
  
  Because $q$ has a smaller degree than $p$, $q(T)v \neq 0$. 
  
  $\implies$ $\lambda$ is an eigenvalue of $T$ with eigenvector $q(T)v$.
  
\end{prf}

\subsection{Eigenvalues and the Minimal Polynomial}
\begin{mydef}
  A monic polynomial is a polynomial whose highest-degree coefficient equals $1$.
\end{mydef}
\begin{example}
  $p(z)=2+9z^2+z^7, \quad \deg p = 7$
\end{example}

\begin{thm}
  \label{unique-monic-polynomial-of-smallest-degree}
  Suppose $T\in \linmap(V)$. Then there exists a unique monic polynomial $p\in \mathcal{P}(\myF)$ of smallest degree such that 
  \begin{equation}
    p(T)=0. 
  \end{equation}
  Furthermore $\deg p \leq \dim V$
\end{thm}
\begin{prf}
  If $\dim V=0$, $I$, is the zero-operator on $V$ and we let $p=1$ such that $1I\vec0=0$.           Now we use induction on $\dim V$ and we assume $\dim V > 0$ and that the theorem holds for all vector spaces of smaller dimension.
  
  Let $v\in V, v \neq 0$. The list \begin{equation}
    v, Tv, \dots, T^{\dim V}
  \end{equation}
  has length $1+\dim V.$
  $\implies$ linear dependence.

  By the linear dependence lemma (\ref{linear-dependence-lemma}), there is a smallest positive integer $m\leq \dim V$ such that
  \begin{equation}
    c_0 v + c_1 Tv + \cdots + c_{m-1} T^{m-1} v + 1\cdot T^m v = 0
  \end{equation}
  for some $c_0, c_1, \dots, c_{m-1} \in \myF$. 

  Let
  \[ q(z) :\equiv c_0 + c_1z + \cdots + c_{m-1} z^{m-1} +z^{m} \in \mathcal{P}_m (\myF) \]
  $\implies q(T) v=0$. Note that $q(z)$ is a monic polynomial.

  If $k \in \nat$, then
  \begin{equation}
    q(T)(T^kv)=T^k(q(T)v) =T^k (0) =0.
  \end{equation}
  By the linear dependence lemma (\ref{linear-dependence-lemma}) \begin{equation}
    \implies v, Tv, \dots, T^{m-1}v
  \end{equation}
  from before are linearly independent \begin{equation}
    \implies \dim \mynull q(T) \geq m
  \end{equation}
  \begin{equation}
    \begin{aligned}
    \implies
    \dim \myrange q(T)
      &= \dim V - \dim \mynull q(T) \\
      &\leq \dim V - m
    \end{aligned}
  \end{equation}
  
  Because $\myrange q(T)$ is invariant under $T$ (by \ref{null-space-and-range-of-p(T)-are-invariant-under-T}), we can apply our induction hypothesis to the operator $\left.T\right|_{\myrange q(T)}$. \\
  So there exists monic $s \in \mathcal{P} (\myF): \deg s \leq \dim V - m$ and $s(\left.T\right|_{\myrange q(T)})=0$ 
  \begin{equation}
    \implies \forall v \in V: (sq)(T)(v) = s(T) (q(T)v) = 0, 
  \end{equation}
  
  because $q(T)v \in \myrange q(T)$ and \begin{equation}
    \left.s(T)\right|_{\myrange q(T)}=s\left( \left.T\right|_{\myrange q(T)} \right ).
  \end{equation}
  Therefore, $sq$ is a monic polynomial such that $\deg sq \leq \dim V$ and $(sq)(T)=0$.

  Proof of uniqueness: Let $p\in \mathcal{P} (\myF)$ a monic polynomial of smallest degree such that 
  \begin{equation}
    p(T)=0.
  \end{equation} Let $r\in \mathcal{P} ( \myF)$ another monic polynomial of same degree such that $r(T)=0.$ 
  \begin{equation}
    \implies (p-r) (T) = 0
  \end{equation} 
  
  and also $\deg (p-r) < \deg p = \deg r$ If $p-r \neq 0$, we could devide $p-r$ by the coefficient of the highest order term in $p-r$ to get a monic polynomial that when applied to $T$ gives the $0$-operator. This polynomial would have a smaller degree than $p$ or $r$, which would be a contradiction. Therefore $p-r=0 \iff p = r$.

  
\end{prf}

\setcounter{thm}{23}

\begin{mydef}
  Let $T\in \linmap (v)$. The ``minimal polynomial of $T$" is the unique monic polynomial $p\in \mathcal{P}(\myF)$ of smallest degree s.t. $p(T)=0$
\end{mydef}
\bfemph{Computation:} Find the smallest $m \in \nat$ such that: \\
$c_0I + c_1 T + \cdots + c_{m-1} T^{m-1} = -T^{m}$ has a solution $c_0, \dots, c_{m-1} \in \myF$. Solve for $m=1,2,\dots,\dim V$

Even faster (usually), pick $v \in V$ with $v \neq 0$ and consider the equation $c_0v + c_1Tv + \cdots + C_{\dim V-1}T^{\dim V-1}v=-T^{\dim V}v$.
If this equation has a unique solution, as happens most of the time $c_0, c_1, c_2, \dots, c_{\dim V-1}, 1$ are the coefficients of the minimal polynomial of $T$.
%TODO: do more.

\setcounter{thm}{26}
\begin{thm}
  \label{thm:zeros-of-the-minimal-polynomial-of-T-are-the-eigenvalues-of-T}
  Let $T \in \linmap(V)$. Then
  \begin{enumerate}[label=(\alph*)]
    \item The zeros of the minimal polynomial of $T$ are the eigenvalues of $T$.
    \item If $V$ is a complex vector space, the minimal polynomial has the form 
    \begin{equation}
      (z-\lambda_1)\cdots(z-\lambda_m), \mytext{where} \lambda_1, \dots, \lambda_m
    \end{equation} are the eigenvalues of $T$, possibly with repetitions.
  \end{enumerate}
\end{thm}
\begin{prf} Let $p$ be the minimal polynomial of $T$.
  \begin{enumerate}[label=(\alph*)]
    \item Suppose $\lambda \in \myF$ is a zero of $p$. $\implies p(z)=(z-\lambda)q(z)$, whre $q$ is a monic polynomial with coefficients in $\myF$ (see \ref{factororing-out-zeros-of-a-polynomial-always-possible})
    \begin{equation}
      p(T)=0\implies 0=(T-\lambda I)(q(T)v) \quad \forall v\in V.
    \end{equation}
    Because $q$ is of lesser degree than $p$, there exists at least one vector $v\in V$ sucht that $q(T)v \neq 0$, which makes $q(T)v$ an eigenvector with eigenvalue $\lambda$.

    Suppose $\lambda \in \myF$ is an eigenvalue of $T$. Thus there exists $v\in V, v \neq 0$ such that $Tv=\lambda v$. Repeated applications of $T$ on both sides of this equation show that $T^kv =\lambda^k v \quad \forall k\in \nat$.
    $\implies p(T)v=p(\lambda)v$. Because $p$ is the minimal polynomial of $T$, we have $p(T)v=0$. $\implies p(\lambda) = 0$. $\implies$ $\lambda$ is a zero of $p$.

    \item use (a) and the second version of the fundamental theorem of algebra. (\ref{fundamental-theorem-of-algebra-second-version})
  \end{enumerate}
\end{prf}

\setcounter{thm}{28}
\begin{thm}[every ``zero-polynomial" is a multiple of the minimal polynomial]
  \label{thm:every-zero-polynomial-is-a-multiple-of-the-minimal-polynomial}
  $T\in \linmap(V)$ and $q \in \mathcal{P} (\myF)$: 
  \begin{equation}
    q(T)=0 \iff q \text{is a multiple of the minimal polynomial of} T.
  \end{equation}
\end{thm}
\begin{prf}
  Let $p$ denote the minimal polynomial of $T$.

	\begin{description}
  
  \item{$\Rightarrow$ direction:}{
			Suppose $q(T)=0$.
			By (\ref{division-algorithm-for-polynomials}) there exists $s,r \in \mathcal{P} (\myF)$ such that
			\begin{equation}
				q=ps+r, \quad \deg r < \deg p
			\end{equation}
			We have
			\begin{equation}
				\label{aa}
				0 = q(T) = p(T)s(T) + r(T) = r(T).
			\end{equation}
			The equation above implies that $r=0$. Otherwise, dividing $r$ by its highest-degree coefficient would produce a monic polynomial that when applied to $T$ gives $0$. A contradiction because $\deg r < \deg p$ and $p$ is minimal. Thus \ref{aa} becomes the equation $q=ps$, as desired
		}
		\item{$\Leftarrow$ direction:}{
			Suppose $q=ps$ for $q,p,s \in \mathcal{P}(\myF)$. We have
			\begin{equation}
				q(T) = p(T)s(T)=0s(T)=0,
			\end{equation}
			as desired. 
      
		}
	\end{description}
  
\end{prf}

\setcounter{thm}{30}
\begin{thm}
  \label{minimal-polynomial-of-a-restriction-operator}
  If $U$ is a subspace of $V$, then the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $\left .T \right | _{ U}$
\end{thm}
\begin{prf}
  Suppose $p$ is the minimal polynomial of $T$.
  \begin{equation}
    \implies p(T)v=0 \quad \forall v \in V.
  \end{equation}
  In particular,
  \begin{equation}
    p(T)u=0 \quad \forall u\in U.
  \end{equation} Thus $p\left( \left.T\right|_{U} \right)=0.$ Now the previous theorem
  \ref{every-zero-polynomial-is-a-multiple-of-the-minimal-polynomial} tells us, that $p$ is a polynomial multiple of the minimal polynomial of $\left. T \right |_U$.
  
\end{prf}

\begin{thm}
  $T \in \linmap (V):$ $T$ is not invertible $\mathsmaller{\iff}$ the constant term of the minimal polynomial of $T$ is $0$.
\end{thm}
\begin{prf}
  $T$ is not invertible $\mathsmaller{\overset{\text{\ref{equivalent-conditions-to-be-an-eigenvalue}}}{\iff}}$ $0$ is an eigenvalue of $T$ $\mathsmaller{\overset{\text{\ref{zeros-of-the-minimal-polynomial-of-T-are-the-eigenvalues-of-T}}}{\iff}}$ $0$ is a zero of $p$ $\mathsmaller{\iff}$ the constant term of $p$ is $0$.
  (In the first equivalence, we have actually used that $0$ is an eigenvalue of $T$ if and only if $T-0I$ is not invertible, according to \ref{equivalent-conditions-to-be-an-eigenvalue}.)
  
\end{prf}

\section{Upper-Triangular Matrices}

\setcounter{thm}{38}
\begin{thm} [conditions for upper-triangular matrix]
  \label{conditions for upper-triangular matrix}
  If $T \in \linmap (V)$ and $v_1, \dots, v_n$ is a basis of $V$. Then the following are equivalent.
  \begin{enumerate}[label=(\alph*)]
    \item The matrix of $T$ with respect to $v_1, \dots, v_n$ is upper triangular.
    \item $\myspan{v_1, \dots, v_k}$ is invariant under $T$ $\quad \forall k \in \{ 1, \dots, n\}$
    \item $T v_k \in \myspan{v_1, \dots, v_k} \quad \forall k \in \{1, \dots, n\}$
  \end{enumerate}
\end{thm}
\begin{prf}
  First suppose (a) holds. So like in \ref{matrix-of-linear-map}, the matrix-diagram $\mmatrix(T)$ looks like this
  \begin{equation}
  \mathcal{M} (T) \equiv
  \begin{blockarray}{cccccc}
             & v_1     & \cdots & v_j      & \cdots & v_n     \\
    \begin{block}{c(ccccc)}
      v_1    & A_{1,1} & \cdots & A_{1,j}  & \cdots & A_{1,n} \\
      \vdots &         & \ddots & \vdots   &   *    & \vdots  \\
      \vdots &         &        & A_{j,j}  &   *    & \vdots  \\
      \vdots &         &        &          & \ddots & \vdots  \\
      v_n    &         &        &          &        & A_{n,n} \\
    \end{block}
  \end{blockarray}
  \end{equation}
  
  Using a modified version of the equation in \ref{matrix-of-linear-map}, we have for $j\in \{1, \ldots n \}$
  \begin{equation}
    T v_j = A_{1,j} v_j + \cdots + A_{j,j} v_j = \sum_{l=1}^{j} A_{l,j} v_j
  \end{equation}
  
  which implies
  \begin{equation}
    T v_j \in \myspan{v_1, \ldots, v_j}
  \end{equation}
  
  Now suppose $j, k \in \{1 \ldots n \}$ such that $j \leq  k.$ Because
  \begin{equation}
    \myspan{v_1, \ldots, v_j} \subseteq \myspan{v_1, \ldots, v_k}
  \end{equation} 
  
  we see that
  \begin{equation}
    T v_j \in \myspan{v_1 \ldots v_k} \mytext{for each} j \in \{1 \ldots k\}.
  \end{equation}

  Thus $\myspan{v_1 \ldots v_k}$ is invariant under $T$, completing the proof that (a) implies (b)
  
  Now suppose (b) holds, so $\myspan{v_1, \ldots, v_k}$ is invariant under $T$ for each $k \in \{1 \ldots n\}.$ 
  
  In particular
  \begin{equation}
    T v_k \in \myspan{v_1, \ldots, v_k} \quad \forall k \in \{1 \ldots n\}
  \end{equation}
  
  Thus (b) implies (c).
  
  Now suppose (c) hods, so $T v_k \in \myspan{v_1, \ldots, v_k} \quad \forall k \in \{1 \ldots n\}$, which is the same as
  \begin{equation}
    T v_k = a_1 v_1 + \cdots + a_k v_k \where a_1, \dots a_k \in \myF
  \end{equation}
  
  Hence all the entries under the diagonal of $\mmatrix (T)$ are $0$, because $v_1, \ldots, v_n$ are linearly independent. Thus $\mmatrix(T)$ is an upper-triangular matrix, completing the proof that (c) implies (a).
  
  We have shown that (a) $\implies$ (b) $\implies$ (c) $\implies$ (a)
  
\end{prf}

\begin{thm}[equation satisfied by operator with upper-triangular matrix]
  \label{thm:equation-satisfied-by-operator-with-upper-triangular-matrix}
  Suppose $T\in \linmap(V)$ and $V$ has a basis with respect to which $T$ has an upper-triangular matrix with diagonal entries $\lambda_1, \dots, \lambda_n$.
  \begin{equation}
    \implies (T-\lambda_1I) \cdots (T-\lambda_nI)=0
  \end{equation}
\end{thm}
\begin{prf}
  Let $A :\equiv \mmatrix(T)$. Let $v_1, \ldots, v_n$ denote a basis of $V$ with respect to which $T$ has an upper-triangular matrix with diagonal entries $\lambda_1, \ldots, \lambda_n$.
  \begin{equation}
    A=\mmatrix(T) =
    \left( {\begin{array}{ccc}
        \lambda_1 &         &  * \\
                  &  \ddots &    \\
            0     &         & \lambda_n
    \end{array} } \right)
  \end{equation}
  
  Then
  \begin{equation}
    \label{i-need-a-ref}
    T v_1 = \lambda_1 v_1 \iff (T-\lambda_1 I)  v_1 = 0,
  \end{equation}

  which implies that
  \begin{equation}
    (T-\lambda_1 I) \cdots (T-\lambda_m I) v_1 = 0, \mytext{for} m = 1, \ldots, n
  \end{equation}
  (using commutativity for linear maps)
  \bigbreak
  Note that $(T-\lambda_2 I) v_2 \in \myspan{v_1}$ because $T v_2 = A_{2,1} v_1 + \lambda_2 v_2$. Thus using (\ref{i-need-a-ref})
  \begin{equation}
    \label{i-also-need-a-ref}
    (T- \lambda_1 I) (T- \lambda_2 I) v_2 = 0.
  \end{equation}
  
  which implies that
  \begin{equation}
    (T-\lambda_1 I) (T-\lambda_2 I) \cdots (T-\lambda_m I) v_2 = 0, \mytext{for} m = 2, \ldots, n
  \end{equation}
  (using commutativity for linear maps)
  \bigbreak
  Note that $(T-\lambda_3 I) v_3 \in \myspan{v_1, v_2}$ because $T v_3 = A_{3,1} v_1 +   A_{3,2} v_2 + \lambda_3 v_3$. 
  Thus using (\ref{i-also-need-a-ref})
  \begin{equation}
    (T- \lambda_1 I) (T- \lambda_2 I) (T- \lambda_3 I)v_3 = 0.
  \end{equation}
  
  which implies that
  \begin{equation}
    (T-\lambda_1 I) (T-\lambda_2 I) \cdots (T-\lambda_m I) v_3= 0, \mytext{for} m = 3, \ldots, n
  \end{equation}
  
  Continuing this pattern, we see that
  \begin{equation}
    (T-\lambda_1 I) \cdots (T- \lambda_n I) v_k = 0 \quad \forall k \in \{ 1\ldots n \}
  \end{equation}
  
  Thus $(T-\lambda_1 I) \cdots (T- \lambda_n I)$ is the $0$ operator because it is $0$ on each vector in a basis of $V$.
  
\end{prf}

\begin{thm}[determination of eigenvalues from upper-triangular matrix]
  \label{thm:determination-of-eigenvalue-from-upper-triangular-matrix}
  Suppose $T\in \linmap(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.

\end{thm}
\begin{prf}
  Let $v_1, \ldots, v_n$ denote a basis of $V$ with respect to which $T$ has an upper-triangular matrix with diagonal entries $\lambda_1, \ldots, \lambda_n$.
  \begin{equation}
    \mmatrix(T) =
    \left( {\begin{array}{ccc}
        \lambda_1 &         &  * \\
                  &  \ddots &    \\
           0      &         & \lambda_n
    \end{array} } \right)
  \end{equation}  
  Because $T v_1 = \lambda_1 v_1$, we see that $\lambda_1$ is an eigenvalue of $T$.
  
  Suppose $k \in \{2 \ldots n\}.$ 
  
  Then $(T-\lambda_k I) v_k \in \myspan{v_1, \ldots, v_{k-1}}.$ 
  
  Thus $T-\lambda_k I$ maps $\myspan{v_1, \ldots, v_k}$ into $\myspan{v_1, \ldots, v_{k-1}}$.
  
  Because
  \begin{equation}
    \dim \myspan{v_1, \ldots, v_k} = k \myand \dim \myspan{v_1, \ldots, v_{k-1}} = k-1,
  \end{equation}
  
  this implies that $T-\lambda_k I$, which is restricted to $\myspan{v_1, \ldots, v_{k-1}}$, is not injective by \autoref{thm: linear-map-to-a-lower-dimensional-space-is-not-injective}. Thus $\exists v \in \myspan{v_1, \ldots, v_n}: v\neq0$ and $(T-\lambda_k I)v=0$. Thus $\lambda_k$ is an eigenvalue of $T$. Hence we have shown that every entry of the diagonal of $\mmatrix(T)$ is an eigenvalue of $T$.
  
  To prove $T$ has no other eigenvalues, let
  \begin{equation}
    \begin{aligned}
      q &:\equiv (z-\lambda_1) \cdots (z-\lambda_2). \\
      &\implies q(T) = 0 \mytext{by \autoref{thm:equation-satisfied-by-operator-with-upper-triangular-matrix}}
    \end{aligned}
  \end{equation}
  
  Hence $q$ is a polynomial multiple of the minimal polynomial of $T$ by \autoref{thm:every-zero-polynomial-is-a-multiple-of-the-minimal-polynomial}. Thus every zero of the minimal polynomial is a zero of $q$.
  
  Because the zeros of the minimal polynomial of $T$ are the eigenvalues of $T$ by \autoref{thm:zeros-of-the-minimal-polynomial-of-T-are-the-eigenvalues-of-T}, this implies that every eigenvalue of $T$ is a zero of $q$. Hence the eigenvalues of $T$ are all contained in the list $\lambda_1, \ldots, \lambda_n$.
  
\end{prf}


\setcounter{thm}{43}
\begin{thm}
  \label{thm:necessary and sufficient condition to have an upper-triangular-matrix}
  Let $T\in \linmap(V)$\footnotemark[1]. Then $T$ has an upper-triangular matrix in respect to some basis $V$ $\iff$ the min. polynomial of $T$ equals $(z-\lambda_1) \cdots (z-\lambda_m), \quad \lambda_1, \dots, \lambda_m \in \myF$
\end{thm}

\begin{thm}
  \label{If-F-equals-C-every-operator-on-V-has-an-upper-triangular-matrix}
  Let $\myF = \compl$. Let $T\in \linmap (V)$\footnotemark[1]. Then $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{thm}

\setcounter{footnote}{1}
\footnotetext{$V$ is finite dimensional.}

\pagebreak

\section{Diagonalizable Operators}
\subsection{Diagonal Matrices}

\setcounter{thm}{47}
\begin{mydef}
  A ``diagonal matrix" is a square matrix that is $0$ everywhere except possibly on \nopagebreak the diagonal
\end{mydef}

\setcounter{thm}{49}
\begin{mydef}
  An operator on $V$ is called ``diagonalizable" if the operator has a diagonal matrix with respect to some basis on $V$
\end{mydef}

\setcounter{thm}{51}
\label{eigenspace}
\begin{mydef}
  Let $T \in \linmap(V)$ and $\lambda \in \myF$. The ``eigenspace" of $T$ corresponding to $\lambda$ is the subspace $E(\lambda, T)$ of $V$ defined by
  \begin{equation}
    E(\lambda, T) :\equiv  \mynull(T-\lambda I) = \{ v\in V \mid Tv = \lambda v\}
  \end{equation}
\end{mydef}

\setcounter{thm}{53}
\begin{thm}
  \label{sum-of-eigenspaces-is-a-direct-sum}
  Suppose $T\in \linmap (V)$ and $\lambda_1, \dots, \lambda_m$ are distinct eigenvalues of $T$. Then
  \begin{equation}
    E(\lambda_1, T) + \cdots + E(\lambda_m, T)
  \end{equation}
  is a direct sum. Furthermore, if $V$ is finite-dimensional, then
  \begin{equation}
    \begin{aligned}
      \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)
      & = \dim \left( E(\lambda_1, T)  \oplus \cdots \oplus E(\lambda_m, T) \right) \\
      & \leq \dim V
    \end{aligned}
  \end{equation}
\end{thm}

\subsection{Conditions for Diagonalizability}
\setcounter{thm}{54}
\begin{thm}
  \label{conditions-equivalent-to-diagonalizability}
  Let $\lambda_1, \dots,\lambda_m$ denote the distinct eigenvalues of $T\in \linmap (V)$. Then
  \begin{enumerate}[label=(\alph*)]
    \item $T$ is diagonalizable.
    \item $V$ has a basis consisting of eigenvectors of $T$.
    \item $V=E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m, T).$
    \item $\dim V = \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)$
  \end{enumerate}
\end{thm}

\setcounter{thm}{57}
\begin{thm}
  \label{enough-eigenvalues-implies-diagonalizability}
  $T\in \linmap(V)$ has $\dim V$ distinct eigenvalues $\implies$ $T$ is diagonalizable.
\end{thm}

\setcounter{thm}{61}
% 5.62
\begin{thm}
  \label{necessary-and-sufficient-condition-for-diagonalizability}
  Let $T\in \linmap (V)$\footnotemark[1]. Then $T$ is diagonalizable $\iff$ the minimal polynomial of $T$ equals
  \begin{equation}
    (z-\lambda_1) \cdots (z-\lambda_m), \where \lambda_1, \dots, \lambda_m \in \myF \myand \lambda_1 \neq \cdots \neq \lambda_m
  \end{equation}
\end{thm}

\setcounter{thm}{64}
%5.65
\begin{thm}
  \label{restriction-ofdiagonalizable-operator-to-invariant-subspace}
  Suppose $T\in \linmap(V)$. $T$ is diagonalizable and $U$ is a subspace of $V$ that is invariant under $T$. $\implies$ $\left.T\right|_U$ is a diagonalizable operator on $U$.
\end{thm}
\begin{prf}
  Diagonazability of $T$ $\mathsmaller{\overset{\text{\ref{necessary-and-sufficient-condition-for-diagonalizability}}}{\iff}}$ the minimal polynomial of $T$ equals 
  \begin{equation}
    (z-\lambda_1)\cdots(z-\lambda_m) \mytext{for} \lambda_1 \neq \cdots \neq \lambda_m.
  \end{equation} 
  
  By \ref{minimal-polynomial-of-a-restriction-operator}, the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $\left.T\right|_U$. Hence the minimal polynomial of $\left.T\right|_U$  has the form required by \ref{necessary-and-sufficient-condition-for-diagonalizability}, which shows that $\left.T\right|_U$ is diagonalizable. It consists of factors $(z-\lambda_1)$ or $(z-\lambda_2), \dots, (z-\lambda_m)$.
  
\end{prf}

% TODO: Gerhgorin Disk Theorem Def 5.66 and Theorem 5.67

\section{Commuting Operators}
\begin{mydef}
  Two operators or matrices $A$ and $B$ ``commute" if $ST=TS$
\end{mydef}