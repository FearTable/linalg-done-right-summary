\section{Bases}

\setcounter{thm}{25}
%\textbf{2.26}
\begin{mydef} [basis]
  A \qt{basis} $\beta$ of $V$ is a list of vectors $\beta = \onetilln{v}$ in $V$ that is linearly independent and spans $V$.
  \begin{equation}
    V = \myspan{v_1, \dots, v_n} = \myspan{ \beta }
  \end{equation}
\end{mydef}

\setcounter{thm}{27}
\begin{thm} [condition for a basis]
  $\onetilln{v}$ is a basis of $V$ $\iff$ every $v \in V$ can be written uniquely in the form
  \begin{equation}
    v=a_1 v_1 + \dots + a_n v_n $, where $ a_1, \dots, a_n \in \myF
  \end{equation}
  Note that the definition of linear dependence has said nothing about uniqueness.
\end{thm}

\setcounter{thm}{29}
\begin{thm} [every spanning list contains a basis]
  \label{thm: every spanning list contains a basis}
  Every spanning list of a \vs can be reduced to a basis of the \vs.
\end{thm}
\begin{prf}
  Suppose $V=\myspan{v_1, \ldots, v_n}.$ Let $B_0 :\equiv (v_1, \ldots, v_n)$.

  \emph{\bfseries Step 1: } If $v_1 = 0$, delete $v_1$ from $B_0$:
  \begin{equation}
    $Let $B_1 :\equiv (v_2, \ldots, v_n).
  \end{equation}

  If $v_1 \neq 0$, then leave $B_0$ unchanged. $B_1 :\equiv B_0=(v_1, \ldots, v_n)$.

  \emph{\bfseries Step k: } If $v_k$ $\in$ $\myspan{v_1^*, \ldots, v_{k-1}^*}$, then delete $v_k$ from list $B_{k-1}$ sucht that:
  \begin{equation}
    B_k :\equiv B_{k-1} - v_k = (v_{1}^*, \ldots, v_{k-1}^*, v_{k+1}, \ldots, v_n)
  \end{equation}

  where $v_1^*, \ldots, v_{k-1}^*$ are not necessarily the original elements $v_1, \ldots, v_{k-1}$ from $B_0$.

  If $v_k \in \myspan{v_1, \ldots, v_{k-1}}$, leave the basis unchanged: $B_k :\equiv B_{k-1}$.

  Stop the process after step $n$, getting a list $B_n$. The list $B_n$ spans $V$ because our original list $B_0$ spanned $V$  have discarded only vectors that were already in the span of the previous vectors. The process ensures that no vector in $B$ is in the span of the previous ones. Thus $B$ is linearly independent. Hence $B$ is a basis of $V$.
  %TODO: Why does axler cite linear dependence lemma 2.19.
\end{prf}

\setcounter{thm}{30}
\begin{thm} [basis of finite\-/ dimensional vector space]
  \label{thm: every finite-dimensional vector space has a basis}
  Every \findimvs has a basis.
\end{thm}
\begin{prf}
  By definition, a \fdvs has a spanning list. The previous result tells us that each spanning list can be reduced to a basis.
\end{prf}

\begin{thm}
  \label{thm: every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space}
  Every \lid list of vectors in a  \findimvs can be extended to a basis of the \vs.
\end{thm}
\begin{prf}
  Let $u_1, \ldots, u_m \in V$ be linearly independent. Let $V=\myspan{w_1, \ddd, w_n}.$
  \begin{equation}
    \implies V=\myspan{u_1, \ddd , u_m, w_1, \ddd , w_n }
  \end{equation}
  Applying the procedure of the proof of \ref{thm: every spanning list contains a basis} to reduce this list to a basis of $V$ produces a basis consisting of vectors $u_1, \cdots, u_m$ and some $w$'s.
\end{prf}

\begin{thm} [every subspace of $V$ is part of a direct sum equal to $V$]
  \label{thm: every subspace of V is part of a direct sum equal to V}
  If $V$ is \fd and $U$ is a subspace of $V$. Then there is another subspace $W$ of $V$ such that $V=U \oplus W.$
\end{thm}
\begin{prf}
  Suppose $U$ is a subspace of a \fdvs $V$.

  $\implies$ $U$ is also \fd (by \ref{thm: finite-dimensional subspace}).

  $\implies$ $\exists u_1, \ldots, u_m \in U $ such that $u_1, \ldots, u_m $ form a basis of $U$. (by \ref{thm: every finite-dimensional vector space has a basis})

  $\implies$ $u_1, \ldots, u_m$ can be extended to a basis $u_1, \ldots, u_m, w_1, \ldots, w_n$ of $V$. (by \ref{thm: every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space})
  \begin{equation}
    $Let $W :\equiv \myspan{w_1, \ldots, w_n}.
  \end{equation}

  To prove $V=U\oplus W$, by $\ref{thm: sum and intersection of two subspaces}$ we only need to show that $V = U+W$ and $U \cap W = \{0\}$.


  \begin{description}

    \item{\bfemph{\slshape Proof of $V=U+W$:}} Let $v \in V$. Since $u_1, \ldots, u_m, w_1, \ldots, w_n$ is a basis of $V$, it spans $V$.
    \begin{equation}
      V = \myspan{u_1, \ldots, u_m, w_1, \ldots, w_n}.
    \end{equation}

    So we have $\exists a_1, \ldots, a_m, b_1, \ldots, b_n \in \myF$ \st
    \begin{equation}
      v = \underbrace{a_1 u_1 + \cdots a_m u_m}_{u} + \underbrace{b_1 w_1 + \cdots b_m w_m}_{w}.
    \end{equation}

    With $u$ and $w$ defined as above, we have $v=u+w$, where $u \in U$ and $w \in W$. Thus $v \in U+W,$ completing the proof that $V=U+W$.

    \item{\bfemph{\slshape Proof of $U \cap W = \{0\}$:}} Let $v \in U \cap W$. $\implies$ $\exists a_1, \ldots, a_m, b_1, \ldots, b_n \in \myF$ \st $v= a_1 u _1 + \cdots + a_m u_m$ and $v= b_1 w_1 + \cdots + b_n w_n$. Thus $a_1 u_1 + \cdots + a_m u_m - b_1 w_1 - \cdots - b_n w_n = 0$.

    Because $u_1, \ldots, u_m, w_1, \ldots, w_n$ is linearly independent, this implies that
    \begin{equation}
      a_1 = \cdots = a_m = b_1 = \cdots= b_n = 0.
    \end{equation}

    Thus $v=0$, completing the proof that $U\cap W = \{0\}.$

  \end{description}
  \vspace{-\baselineskip}
\end{prf}