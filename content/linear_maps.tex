\chapter{Linear Maps}
\section{\vs of linear maps}

\begin{mydef}
    A \lm from $V$ to $W$ is a function $T:V\to W$ with following properties:
    \begin{itemize}
    	\item additivity: $T(u+v)=Tu + Tv \qquad \forall u,v \in V$
    	\item homogenity: $T(\lambda v)=\lambda (Tv) \qquad
    	\forall \lambda \in \mathbb{F} \; \forall v\in V$
    \end{itemize}
\end{mydef}

\begin{mydef}
    The set of all linear maps from $V$ to $W$ is denoted by $\mathcal{L}(V,W)$. For The set of all linear maps from $V$ to itself we use $\mathcal{L}(V) :\equiv \mathcal{L}(V,V)$
\end{mydef}

\setcounter{thm}{3}
\begin{thm}
    \textbf{Linear map lemma: }
    Suppose $\onetilln{v}$ is a basis of $V$ and $\onetilln{w} \in W$, where the $w$'s do not have any specific properties. Then there exist a unique \lm $T:V\to W$ or $T \in \linmap(V,W)$ such that
    \begin{itemize}
       	\item[] $Tv_{k} = w_k \quad \forall \kinonetilln.$
    \end{itemize}
    It is of the form $T(c_1 v_1 + \cdots c_n v_n) \mapsto c_1 w_1 + \cdots + c_n w_n$
\end{thm}

\setcounter{thm}{4}
\begin{mydef}
    The sum and the product of linear maps:\\
    If $S, T \in \lvw$, $v \in V$, $\lambda \in \mathbb{F} $:
    \begin{itemize}
    	\item $(S+T)(v) :\equiv Sv+Tv$
    	\item $(\lambda T)(v) : \equiv \lambda (Tv)$
    \end{itemize}
\end{mydef}

\setcounter{thm}{5}
%\textbf{3.6}
\begin{thm}
    With these operations above, $\lvw$ is itself a \vs.
\end{thm}

\setcounter{thm}{6}
%\textbf{3.7}
\begin{mydef}
    Let $T \in \lin{U}{V}$ and $S \in \lin{V}{W}$. We define the product $ST \in \lin{U}{W}$ as follows:
    \begin{itemize}
    	\item[] $(ST)(u) :\equiv S(Tu) \quad \forall u \in U$
    \end{itemize}
\end{mydef}

%\textbf{3.8}
\begin{thm}
    With these definitions we have
    \begin{itemize}
    	\item \bfemph{Associativity:} $(T_1 T_2) T_3 = T_1 (T_2 T_3)$, whenever $T_3$ maps into the Domain of $T_2$ and $T_2$ maps into the Domain of $T_1$.
    	\item \bfemph{Identity:} $T I = I T = T$ for $T \in \lvw$ (The first $I$ is the identity operator on $V$ and the second $I$ the identity operator on $W$. We could also write $T I_V = I_W T$
    	\item \bfemph{Distributive properties:} For $T, T_1, T_2 \in \lin{U}{V}$ and $S, S_1, S_2 \in \lin{V}{W}$: \\ $(S_1 + S_2)T=S_1 T + S_2 T$ and $S(T_1 + T_2)=S T_1 + S T_2$
    	\item \bfemph{Non-commutative:} $ST \neq TS$ in general.
    \end{itemize}
\end{thm}

\setcounter{thm}{9}
\begin{thm}
     $T\in \lvw \implies T(0)=0$.
\end{thm}
\begin{proof}
    $T(0) = T(0+0) = T(0) + T(0)$. Subtracting $T(0)$ on both sides ends the proof.
\end{proof}

\pagebreak

\section{Null spaces, ranges and injectivity}

\begin{mydef}
    $\mynull T :\equiv \ker T :\equiv \{ v \in V \mid Tv=0\} \subseteq V$ for $T \in \lvw$
\end{mydef}

\setcounter{thm}{12}
%\textbf{3.13}
\begin{thm}
    $\mynull T$ is a subspace of $V$
\end{thm}

\setcounter{thm}{13}
%\textbf{3.14}
\begin{mydef}
    A function $T: V \to W$ is called injective if $Tu = Tv \implies u = v$. \\
Or analogous: $u \neq v \implies Tu \neq Tv$
\end{mydef}

\setcounter{thm}{14}
%\textbf{3.15}
\begin{mydef}
    Let $T \in \lvw$. Then $T$ is injective $\iff \mynull T = \{0 \}$
\end{mydef}

\section{Definition of Range}
\setcounter{thm}{15}
%\textbf{3.16}
\begin{thm}
    $\operatorname{range}T= \{Tv \mid v \in V\} \subseteq W$ for $T \in \lvw$
\end{thm}

\setcounter{thm}{17}
%\textbf{3.18}
\begin{thm}
    $T\in \lvw \implies \myrange T$ is a subspace of $W$.
\end{thm}

\setcounter{thm}{18}
%\textbf{3.19}
\begin{mydef}
    If $\myrange T=W$, $T$ is called ``surjektive" or ``onto".
\end{mydef}

\setcounter{thm}{20}
%\textbf{3.21}
\begin{thm}
    \label{rank-nullity-theorem}
    Fundamental theorem of linear maps or \textbf{``rank nullity theorem":} \\
    For $T \in \lvw$:
    \begin{equation}
    	\dim V =
    	\underbrace{ \dim \mynull T }_{\text{nullity}}
    	+ \underbrace{\dim \myrange T}_{\text{rank}}
    \end{equation}
\end{thm}

\setcounter{thm}{21}
%\textbf{3.22}
\begin{thm}
    If $\dim V > \dim W \implies$ No \lm from $V$ to $W$ is injective.
\end{thm}

\setcounter{thm}{23}
%\textbf{3.24}
\begin{thm}
    If $\dim V < \dim W \implies$ No \lm from $V$ to $W$ is surjective.
\end{thm}

\setcounter{thm}{25}
%\textbf{3.26}
\begin{thm}
    A homogeneous system of linear equations with more variables then equations has nonzero solutions.
\end{thm}

%\textbf{3.28}
\setcounter{thm}{27}
\begin{thm}
	An inhomogeneous system of linear equations with more equations then variables has no solutions for some choice of constant terms.
\end{thm}

\pagebreak

\section{Matrices}
\subsection{Representing a Liner Map by a Matrix}

\setcounter{thm}{30}
\begin{mydef}
    \label{matrix-of-linear-map}
    \textbf{Matrix of a linear map, $\mathbf{\mmatrix(T)}$:} \\
    Suppose $T \in \linmap (L,W)$ and $v_1, \dots, v_n$ be a basis of $V$ and $w_1, \dots, w_m$ be a basis of $W$. The ``matrix of $T$" with respect to these bases is the $m$-by-$n$ matrix $\mmatrix(T)$ whose entries $A_{j,k}$ are defined by
    \begin{equation}
        T v_k = A_{1,k} w_1 + \cdots + A_{m,k} w_m = \sum_{j=1}^{m} A_{j,k} w_j
    \end{equation}
    If the bases are not clear from the context, one writes $\mmatrix(T, (v_1, \dots, v_n),(w_1, \dots, w_m))$. One remembers how to construct $\mmatrix(T)$ as follows:
    \begin{equation}
        \mathcal{M} (T) :\equiv
        \begin{blockarray}{cccc}
            & v_1 \quad \cdots & v_k & \cdots \quad v_n \\
            \begin{block}{c(ccc)}
                w_1    & & A_{1,k} & \\
                \vdots & & \vdots & \\
                w_m    & & A_{m,k} & \\
            \end{block}
        \end{blockarray}
    \end{equation}
\end{mydef}

\subsection{Addition and Scalar Multiplication of Matrices}
\setcounter{thm}{34}
\begin{thm}
    $\mmatrix(S+T) = \mmatrix(S) + \mmatrix(T)$ for $S, T \in \linmap (V,W)$
\end{thm}

\setcounter{thm}{37}
\begin{thm}
    $\mmatrix(\lambda T) = \lambda \mmatrix(T)$ for $\lambda \in \myF$ and $T \in \linmap (V,W)$
\end{thm}

\begin{mydef}
    $\myF^{m,n}$ denotes the set of all $m$-by-$n$ matrices with entries in $F$, where $m,n \in \nat$
\end{mydef}

\begin{thm}
    $\myF^{m,n}$ is a vector space with $\dim \myF^{m,n}=mn$.
\end{thm}



\setcounter{thm}{40}
%\textbf{3.41}

\begin{mydef}
    $(AB)_{j,k} :\equiv \sum_{r=1}^{m} A_{j,r} B_{r,k}$ for $A\in \myF^{m,n}$, $B\in \myF^{n,p}$, $AB \in \myF^{m,p}$
\end{mydef}

%\textbf{3.46}
\setcounter{thm}{45}
\begin{thm}
    $(AB)_{j,k} = A_{j, \mathsmaller{\bullet}} B_{\mathsmaller{\bullet}, k}$ if $1 \leq j \leq m$ and $1 \leq k \leq p$
\end{thm}

%\textbf{3.31}
\setcounter{thm}{30}
\begin{thm}
    Supp $T \in \lvw$ and let $\onetilln{v}$ be a basis of $V$ and $\onetillm{w}$ be a basis of $W$. The matrix of T with respect of these bases is the $m$-by-$n$ Matrix called $\mathcal{M}(T) :\equiv A$ whose entries $A_{j,k}$ are defined by
	\begin{equation}
		Tv_k :\equiv A_{1,k}w_1+\cdots+A_{m,k}w_m
	\end{equation}
	also denoted as $\mathcal{M}(T,(\onetillm{v}), (\onetillm{w})).$
\end{thm}

\setcounter{thm}{49}
\begin{thm}
    Suppose $A \in \myF^{m,n}$ and $b=\left(\begin{matrix}b_1\\ \vdots \\ b_n \end{matrix}\right) \in \myF^{n,1}$. Then $Ab = b_1 A_{\mathsmaller{\bullet}, 1} + \dots + b_n A_{\mathsmaller{\bullet},n}$
\end{thm}

\section{Invertibility and Isomorphisms}

\subsection{Inverse}
\setcounter{thm}{58}
%\textbf{3.59}
\begin{mydef}
    $S\in \lvw$ with $ST=I_V \wedge TW = I_W$ is called the ``inverse" (map?) of the invertible linear map $T \in \lvw$
\end{mydef}

%\textbf{3.60}
\begin{thm}
    An invertible  linear map has a unique inverse.
\end{thm}
\begin{proof}
    Suppose $T\in \lvw$ is invertible and $S_1$ and $S_2$ are inverses of $T$. $\implies S_1 = S_1 I = S_1 (T S_2) = (S_1 T) S_2 = I S_2 = S_2 \implies S_1 = S_2$
\end{proof}

%\textbf{3.61}
\begin{thm}
    The inverse is denoted by $T^{-1}$. $T^{-1}T=I_V$ and $T T^{-1} = I_W$ (?)
\end{thm}

%\setcounter{thm}{62}
%\textbf{3.63}
\begin{thm}
    Invertibility of a linear map $\in \lvw$ $\Leftrightarrow$ injectivity and surjectivity.
\end{thm}

\setcounter{thm}{64}
%\textbf{3.65}
\begin{thm}
    \label{injectivity-is-equivalent-to-surjectivity}
    If $\dim V = \dim W \neq \infty$, injectivity is equivalent to surjectivity for $T\in \linmap(V,W):$

    $T$ is invertible $\iff$ $T$ is injective $\iff$ $T$ is surjective.
\end{thm}
\begin{proof}
    The rank-nullity theorem (\ref{rank-nullity-theorem}) or fundamental theorem of linear maps states that
    \begin{equation}
        \label{rank-nullity-nested-equation}
        \dim V = \dim \mynull T + \dim \myrange T
    \end{equation}

    If $T$ is injective, $\dim \mynull T = 0$ and therefore $\dim \myrange T = \dim V = \dim W$, which means $T$ is surjective ($\myrange T = \dim W$). This is because every linearly independent list of vectors of of length $\dim W$ is a basis by \ref{every-lid-list-of-length-dim-v-is -a-basis-of-v}.

    If $T$ is surjective, we have $\dim \myrange T = \dim W$ to begin with and therefore \ref{rank-nullity-nested-equation} becomes
    \begin{equation}
        \dim \mynull T = \dim V - \dim \myrange T = \dim V - \dim W = 0
    \end{equation}
    which makes $T$ also injective. Since injectivity and surjectivy together imply invertibility, this ends the proof.
\end{proof}


\setcounter{thm}{67}
%\textbf{3.68}
\begin{thm}
    $ST = I \Leftrightarrow TS=I$
\end{thm}

%\textbf{3.69}
\begin{thm}
    An isomorphism is an invertible linear map. Two vector spaces are called isomorphic if there is a isomorphism from one vector space to the other. $T:V\to W$
\end{thm}

%\textbf{3.70}
\begin{thm}
    Two \fd vector spaces $U$, $V$ over $\mathbb{F}$ are isomorphic $\iff$ they have the same dimension. $\dim U = \dim V$
\end{thm}

%\textbf{3.71}
\begin{thm}
    Suppose $\oneTillN{v}$ is a basis of $V$ and $\oneTillM{w}$ is a basis of $W$. Then $\mathcal{M}$ is an isomorphism between $\lvw$ and $\mathbb{F}^{m,n}$
\end{thm}

%\textbf{3.72}
\begin{thm}
    Suppose $V,W$ are \fd. $\implies \lvw$ is \fd and
    \begin{equation}
    	\dim \lvw = (\dim V)\cdot(dim W)
    \end{equation}
\end{thm}

\subsection{Linear maps thought of as matrix multiplication}

\begin{mydef}
    Let $v \in V$ and $\onetillm{v}$ be a basis of $V$ such that $v=b_1v_1+\dots+b_mv_m$.
    \\
    We define
    $
    	\mathcal{M}(v) :\equiv
    	\left (
    	\begin{matrix}
    		b_1 \\ \vdots \\ b_m
    	\end{matrix}
    	\right )
    $. The vector depends on the basis, but it is not included in the notation.
\end{mydef}

Recall: $
 \mathcal{M} (T) :\equiv
\begin{blockarray}{cccc}
	& v_1 \quad \cdots & v_k & \cdots \quad v_n \\
	\begin{block}{c(ccc)}
		w_1    & & A_{1,k} & \\
		\vdots & & \vdots & \\
		w_m    & & A_{m,k} & \\
	\end{block}
\end{blockarray}
$


\setcounter{thm}{74}
%\textbf{3.75}
\begin{thm}
    $\mathcal{M}_{\mathsmaller{\bullet}, k} = M(T v_k)$. The $k^{\text{th}}$ column of $\mathcal{M}(T)$ equals $(A_{1,k}, \dots, A_{m,k})^\top$
\end{thm}

%\textbf{3.76}
\begin{thm}
    Linear maps act like matrix multiplication: Suppose $T\in \lvw$ and $v\in V$. Suppose $\onetilln{v}$ is a basis of $V$ and $\onetillm{w}$ is a basis of $W$. Then
    \begin{equation}
    	\mathcal{M} (Tv)=\mathcal{M}(T) \cdot \mathcal{M}(v)
    \end{equation}

    Or using different notation:
    \begin{equation}
\mathcal{M} (Tv, (\onetilln{v}), (\onetillm{w}))=\mathcal{M}(T, (\onetilln{v}), (\onetillm{w})) \cdot \mathcal{M}(v)
    \end{equation}

\end{thm}

\setcounter{thm}{77}
%\textbf{3.78}
\begin{thm}
    For $T \in \lvw: \dim \myrange T = \text{column rank of } \mathcal{M} (T)$
\end{thm}

\subsection{Change of basis}

\begin{mydef-non}
    $\mathcal{M}(T, (\onetilln{v})) :\equiv \mathcal{M}(T, (\onetilln{v}),(\onetilln{v}))$
\end{mydef-non}

\setcounter{thm}{79}
%\textbf{3.80}
\begin{thm}
    A square matrix $A$ is called invertible, if there is some square matrix B of the same size such that $AB=BA=I$. We call $B$ the inverse of $A$ denoted by $A^{-1} :\equiv B$. Rules:
    \begin{itemize}
    	\item $(A^{-1})^{-1}=A$
    	\item $(AC)^{-1} = C^{-1}A^{-1}$ (Because $(AC)(C^{-1}A^{-1})=I$ and $(C^{-1}A^{-1})(AC)=I$)
    \end{itemize}
\end{thm}

%\textbf{3.81}
\begin{thm}
    \bfemph{Matrix of product of linear maps:} \\
    Suppose $T\in \mathcal{L}(U,V)$ and $S\in \lvw$. If $\onetillm{u}$ is a basis of $U$, $\onetilln{v}$, is a basis of $V$ and $\onetill{w}{p}$ is a basis of $W$. Note that $\dim U = m$, $\dim V = n$, $\dim W = p$. Then we have:
    \begin{multline}
    	\mmatrix(ST, (\onetillm{u}), (\onetill{w}{p})) = \\
    	\mmatrix(S, (\onetilln{v}), (\onetill{w}{p})) \cdot \\
    	\mmatrix(T, (\onetillm{u}), (\onetilln{v}))
    \end{multline}

    Or using different notation:
    \begin{equation}
        \mathcal{M}(ST) = \mathcal{M}(S) \cdot \mathcal{M}(T)
    \end{equation}
\end{thm}

\setcounter{thm}{83}
%\textbf{3.84}
\begin{thm}
    \bfemph{Change-of-basis formula:}\\
    Suppose $T \in \linmap(V)$. Let
    $V = \myspan{\onetillm{u}} = \myspan{\onetillm{v}}$ such that the $u$'s and $v$'s both form a basis. Let $A=\mmatrix(T, (\onetillm{u}))$ and $B=\mmatrix(T,(\onetillm{v}))$. Let
    $C=\mmatrix(I, \onetillm{u}, \onetillm{v})$. Then
    \begin {equation}
        A = C^{-1} B C
    \end {equation}
\end{thm}

\setcounter{thm}{85}
%\textbf{3.86}
\begin{thm}
    If $\onetillm{v}$ is a basis $V$ and $T\in \mathcal{L}$ is invertible, then
    \begin{equation}
    	\begin{aligned}
    		\mmatrix(T^{-1}) & = (\mmatrix(T))^{-1} \; \text{or} \\
    		\mmatrix(T^{-1}, (\onetillm{v})) & =\mmatrix(T, (\onetillm{v}))^{-1}
    	\end{aligned}
    \end{equation}
\end{thm}

\filbreak
\section{Products and quotients of vector spaces}
\subsection{Products of vector spaces}

\begin{mydef}The product, addition and scalar multiplication of vector spaces $V_1, \cdots, V_m$ is defined as follows:
    \begin{equation}
	\begin{aligned}
		V_1 \times \cdots \times V_m &:\equiv \{ (v_1, \dots, v_m) \mid v_1 \in V_1, \dots, v_m \in V_m\} \\
		(u_1, \dots, u_m) + (v_1, \dots, v_m) &:\equiv (u_1+v_1, \dots, u_m+v_m) \\
		\lambda (v_1, \dots, v_m) &:\equiv (\lambda v_1, \dots, \lambda v_m)
	\end{aligned}
\end{equation}
\end{mydef}

\setcounter{thm}{88}
%\textbf{3.89}
\begin{thm}
    $V_1 \times \cdots \times V_m$ together with addition and scalar multiplication is a vector space over $\mathbb{F}$.
\end{thm}

\setcounter{thm}{91}
\begin{thm}
    $\dim (V_1 \times \cdots \times V_m) = \dim V_1 + \cdots + \dim V_m$
\end{thm}

%\textbf{3.93}
\begin{thm}
    Let $\Gamma: V_1 \times \cdots \times V_m \to V_1 + \cdots + V_m$ and
    $ \Gamma(v_1, \cdots, v_m) \mapsto v_1 + \cdots + v_m$ \\
Then $v_1 + \cdots + v_m$ is a direct sum $\iff$ $\Gamma$ is injective.
\end{thm}

\begin{thm}$V_1 + \cdots + V_1$ is a direct sum $\iff$
$\dim (V_1+\cdots+V_m) = \dim V_1 + \cdots + \dim V_m$
\end{thm}

\section{Quotient spaces}

\setcounter{thm}{94}
%\textbf{3.95, 3.97}
\begin{mydef}
    $v+U :\equiv \{v+U \mid u\in U\}$ for $v\in V$ and $U\subseteq V$ is said to be ``a translate" of $U$.
\end{mydef}

\setcounter{thm}{98}
%\textbf{3.99}
\begin{mydef}
    $V/U :\equiv \{v+U \mid v\in V\}$ is called ``quotient space".
\end{mydef}

\begin{example}
    If $U=\{ (x,2x)\in \mathbb{R}^2 \mid x\in \mathbb{R} \} \implies \mathbb{R}/U$ is the set of all lines with slope $2$.
\end{example}

\begin{example}
    If $U$ is a plane in $\mathbb{R}^3$ $\implies$ $\mathbb{R}^3/U$ is the set of all planes parallel to $U$.
\end{example}


\setcounter{thm}{100}
%\textbf{3.101}
\begin{thm}
    $ U \subseteq V$ and $v,w\in V$. Then
$$v-w \in U \iff v+U = w + U \iff (v+U) \cap (w+U) \neq \varnothing$$
That means, two translates of a subspaces are equal or disjoint.
\end{thm}

%\textbf{3.102}
\begin{mydef}
    Definition of addition and scalar multiplication on $V/U$.
\begin{equation}
	\begin{aligned}
		(v+U)+(w+U) & :\equiv (v+w) + U \\
		\lambda (v+U) & :\equiv (\lambda v) + U \qquad \forall v,w \in V \text{ and } \forall \lambda \in \mathbb{F}
		\text{ and } U \subseteq V
	\end{aligned}
\end{equation}
\end{mydef}


%\textbf{3.103}
\begin{thm}
    $V/U$ is a vector space with additive identity $0+U$ which is equal to $U$ and the additive inverse $(-v)+U$.
\end{thm}

\textbf{3.104} TODO \textbf{3.105} TODO \\

 \filbreak
\section{Duality}
\subsection{Dual Space and Dual Map}

\setcounter{thm}{107}
%\textbf{3.108}
\begin{mydef}
    A ``linear functional" $\phi$ is an element of $\mathcal{L}(V, \mathbb{F})$. So $\phi \in \linmap(V, \myF)$
\end{mydef}

\textbf{Examples:}
\begin{equation}
	\begin{array}{lll}
		\phi: \mathbb{R}^3  \to \mathbb{R}, &\phi (x,y,z)  & \mapsto 4x-5y-2z \\
		\phi: \mathbb{F}^n  \to \mathbb{F}, &\phi (x_1, \dots, x_n)
		& \mapsto c_1x_1 + \dots + c_nx_n  \\
		\phi: \mathcal{P} (\mathbb{R})  \to \mathbb{R},
		& \phi(p) & \mapsto 3p''(5) + 7p(4) \\
		\phi: \mathcal{P}(\mathbb{R}) \to \mathbb{R},
		& \phi(p)  &\mapsto \textstyle \int_{0}^{1} p(x) dx
	\end{array}
\end{equation}

\setcounter{thm}{109}
%\textbf{3.110}
\begin{thm}
    The dual space of $V$, denoted by $V'$ or $V^{*}$, is the vector space of all linear functionals on $V$.
    \begin{equation}
        V^{*} :\equiv \linmap(V, \myF)
    \end{equation}
\end{thm}

%\textbf{3.111}
\begin{thm}
    $\dim V^{*} = \dim V$.
\end{thm}
\begin{proof}
    $\dim V^{*} = \dim \mathcal{L}(V, \mathbb{F})=(\dim V) \cdot (\dim \mathbb{F}) = \dim V $
\end{proof}


%\textbf{3.112}
\begin{mydef}
    If $\onetillm{v}$ is a basis of $V$, then the ``dual basis" of $\onetilln{v}$ is the list $\varphi_1, \dots, \varphi_m \in V^{*}$, where each $\varphi_j$ is the linear functional such that
$\varphi_j(v_k) = \delta_{j,k} =
\begin{cases}
	1,  & \text{if $k=j$} \\
	0, & \text{if $k \neq j$}
\end{cases}$. \\
Note that this is not the definition of each $\varphi_j$.
\end{mydef}

%\textbf{3.114}
\setcounter{thm}{113}
\begin{thm}
    Suppose $\onetillm{v}$ is a basis of $V$. $\onetillm{\varphi}$ is called a dual basis of $\onetillm{v}$ because for every $v \in V$ we have $v=\varphi_1 (v)v_1 + \cdots \varphi_m(v)v_m$
\end{thm}
\begin{proof}
    Let $v\in V$, $v= c_1v_1 + \cdots c_mv_m$. If $j\in \{1, \dots, m\}$, then applying
    $\varphi_j$ on both sides gives $\varphi_j(v)=c_j$.
%    \iff$ $\varphi(v)=\phi(c_1v_1 + \cdots + c_mv_m)$ such that $\varphi_j(v)=c_j$, because $\varphi$ is linear and $\varphi_j(v_j)=1$ by definition.
\end{proof}

%\textbf{3.116}
\setcounter{thm}{115}
\begin{thm}
    The dual basis of a basis of $V$ is a basis of the dual space $V^{*}$
\end{thm}

%\textbf{3.118}
\setcounter{thm}{117}
\begin{mydef}
    Suppose $T \in \lvw$. The ``dual map" of $T$ is the linear map $T^{*} \in \lin{W^{*}}{V^{*}}$ defined like this:

    \begin{itemize}
    \item[] $\forall \phi \in W^{*}: T^{*}(\varphi) :\equiv \varphi \circ T $

    \item[]

    \item $\varphi \in W^{*}=\lin{W}{\mathbb{F}} \text{ and } T^{*}(\varphi) \in V^{*} = \lin{V}{\mathbb{F}}$.
    So $T^{*}$ is indeed a map from $W^{*}$ to $V^{*}$


    	\item $\varphi, \psi \in W^{*} \implies T^{*} (\varphi + \psi) = (\varphi + \psi) \circ T = \varphi \circ T + \psi \circ T = T^{*} (\varphi) + T^{*} (\psi)$
    	\item $\lambda \in \mathbb{F}, \varphi \in W \implies T^{*} (\lambda \varphi) = (\lambda \varphi) \circ T = \lambda (\varphi \circ T) = \lambda T^{*} (\varphi)$
    \end{itemize}
\end{mydef}

