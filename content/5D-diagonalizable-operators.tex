\clearpage
\section{Diagonalizable Operators}
\subsection{Diagonal Matrices}

\setcounter{thm}{47}
\begin{mydef} [diagonal matrix]
  A \qt{diagonal matrix} is a square matrix that is $0$ everywhere except possibly on \nopagebreak the diagonal.
\end{mydef}

\setcounter{thm}{49}
\begin{mydef} [diagonalizable]
  An operator on $V$ is called \qt{diagonalizable} if the operator has a diagonal matrix with respect to some basis of $V$.
\end{mydef}

\setcounter{thm}{51}
\label{eigenspace}
\begin{mydef} [eigenspace, $E(\lambda, T)$]
  Let $T \in \linmap(V)$ and $\lambda \in \myF$. The \qt{eigenspace} of $T$ corresponding to $\lambda$ is the subspace $E(\lambda, T)$ of $V$ defined by
  \begin{equation}
    E(\lambda, T) :\equiv  \mynull(T-\lambda I) = \{ v\in V \mid Tv = \lambda v\}.
  \end{equation}
\end{mydef}

\setcounter{thm}{53}
\begin{thm} [sum of eigenspaces is a direct sum]
  \label{thm: sum of eigenspaces is a direct sum}
  Suppose $T\in \linmap (V)$ and $\lambda_1, \dots, \lambda_m$ are distinct eigenvalues of $T$. Then
  \begin{equation}
    E(\lambda_1, T) + \cdots + E(\lambda_m, T) $ is a direct sum.$
  \end{equation}
  Furthermore, if $V$ is finite-dimensional, then
  \begin{equation}
    \begin{aligned}
      \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)
      & = \dim \left( E(\lambda_1, T)  \oplus \cdots \oplus E(\lambda_m, T) \right) \\
      & \leq \dim V
    \end{aligned}
  \end{equation}
\end{thm}
\begin{prf}
  Let $\lambda_1, \ddd, \lambda_m$ be distinct eigenvalues. Suppose $v_1 + \cdots + v_m = 0$, where each $v_k \in E(\lambda_k, T)$ for $k = 1, \ddd, m $. This means $\forall k \in \{ 1, \ddd, m\}: v_k = 0$, because eigenvectors corresponding to distinct eigenvalues are linearly independent by \ref{thm: linearly independent eigenvectors}. Thus, $E(\lambda_1, T) + \cdots + E(\lambda_m, T)$ is a direct sum by \ref{thm: condition for a direct sum}.
  By \ref{thm: a sum is a direct sum if and only if the dimensions add up}, the dimensions of a direct sum add up. By \ref{thm: dimension of a subspace}, the dimension of a subspace is always smaller or equal to the space itself, if it is \fd. Therefore,
  \begin{equation}
      \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)
    = \dim \left( E(\lambda_1, T)  \oplus \cdots \oplus E(\lambda_m, T) \right) \leq \dim V
  \end{equation}
  \vspace{-1em}
\end{prf}

\subsection{Conditions for Diagonalizability}
\setcounter{thm}{54}
\begin{thm} [conditions equivalent to diagonalizability]
  \label{thm: conditions equivalent to diagonalizability}
  Let $\lambda_1, \dots,\lambda_m$ denote the distinct eigenvalues of $T\in \linmap (V)$, so $\lambda_j \neq \lambda_k$. Then the following are equivalent:
  \begin{enumerate}[label=\textbf{(\alph*)}]
    \item $T$ is diagonalizable.
    \item $V$ has a basis consisting of eigenvectors of $T$.
    \item $V=E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m, T).$
    \item $\dim V = \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T)$.
  \end{enumerate}
\end{thm}
\begin{prf}
  \StepOne Let $\lambda_1, \ddd, \lambda_m$ be distinct eigenvalues of $T \in \linmap (V)$ for the whole proof. First, suppose (a) holds.
  \begin{equation}
    \implies
    \mmatrix(T) =
    \left( {\begin{array}{ccc}
        \lambda_1 &         & 0 \\
        &  \ddots &    \\
        0      &         & \lambda_n
    \end{array} } \right)
  \end{equation}

  with respect to a basis $v_1, \ddd, v_n$. This is the case $\iff$ $Tv_k = \lambda_k v_k$ for $k = 1, \ddd, n$. Thus (a) and (b) are equivalent, i.e. (a) $\iff$ (b).

  \StepTwo Now, suppose (b) holds; thus $V$ has a basis consisting of eigenvectors of $T$. Hence, every vector in $V$ is a linear combination of such basis, which implies
  \begin{equation}
    V = E(\lambda_1, T) + \cdots + E(\lambda_m, T).
  \end{equation}

  \ref{thm: sum of eigenspaces is a direct sum} shows that this is a direct sum. This proves that (b) implies (c).

  \StepThree That (c) implies (d) follows immidiately from \ref{thm: a sum is a direct sum if and only if the dimensions add up}, which states that the dimensions of a direct sum add up.

  \StepFour Finally, suppose (d) holds; thus
  \begin{equation}
    \dim V = \dim E(\lambda_1, T) + \cdots + \dim E(\lambda_m, T).
  \end{equation}

  Choose a basis of each $E(\lambda_m, T)$; put all these together to form a list $v_1, \ddd, v_n$ of eigenvectors of $T$, where $n = \dim V$. Suppose $a_1 v_1 + \cdots + a_n v_n = 0$, where $a_1, \ddd, a_n \in \myF$.
  Now $\forall k \in \{1, \ddd, m \}$, let
  \begin{equation}
    u_k := \hspace{-1.5em} \sum_{\substack{v_j \in E(\lambda_k, T) \\ \text{$v_j$ is a basis vector}}} \hspace{-1.5em} (a_j v_j)
  \end{equation}

  denote the $k$-th sum of the terms $a_jv_j$, where the $v_j$'s belong to the basis of $E(\lambda_k, T)$. This means $u_k \in E(\lambda_k, T)$ is an eigenvector with eigenvalue $\lambda_k$.
  Thus, $\forall k \in \{1, \ddd, m \}$:
  \begin{equation}
    u_k \in E(\lambda_k, T)$ and $u_1 + \cdots + u_m = 0.
  \end{equation}

  Since by \ref{thm: linearly independent eigenvectors},
  every list of eigenvectors of $T$, in this case $u_1, \ddd, u_m$, corresponding to distinct eigenvalues of $T$, in this case $\lambda_1, \ddd, \lambda_m$, is linearly independent, it follows that
  \begin{equation}
    u_k = 0 \quad \forall k \in \{1, \ddd, m \}.
  \end{equation}

  $\implies$ all $a_j$'s $=0$, because the $v$'s are bases of the $E$'s  (eigenspaces), and their corresponding sums (the $u_k$'s) all equal to $0$. Thus $v_1, \ldots, v_m$ is a linearly independent list and hence a basis of $V$ by \ref{thm: linearly independent list of the right length is a basis}. Thus (d) implies (b), completing the proof.
\end{prf}

\setcounter{thm}{57}
\begin{thm} [enough eigenvalues implies diagonalizability]
  \label{thm: enough eigenvalues implies diagonalizability}
  $T\in \linmap(V)$ has $\dim V$ distinct eigenvalues
    $\implies$ $T$ is diagonalizable.
\end{thm}
\begin{prf}
  Let $\lambda_1, \ldots, \lambda_{\dim V}$ be the eigenvalues of the eigenvectors $v_1, \ldots, v_{\dim V}$, which are linearly independent by \autoref{thm: linearly independent eigenvectors}. So we have a basis consisting of $\dim V$ eigenvectors. So $T$ is diagonalizable by \ref{thm: conditions equivalent to diagonalizability}
\end{prf}

\setcounter{thm}{61}
% 5.62
\begin{thm}[necessary and sufficient condition for diagonalizability]
  \label{thm: necessary and sufficient condition for diagonalizability}
  Let $T\in \linmap (V)$, $\dim V \neq \infty$. Then $T$ is diagonalizable $\iff$ the minimal polynomial of $T$ equals
  \begin{equation}
    (z-\lambda_1) \cdots (z-\lambda_m), \where \lambda_1, \dots, \lambda_m \in \myF \myand \lambda_i \neq  \lambda_j $ for $ i,j \in \{1, \ddd, m \}.
  \end{equation}
\end{thm}
\begin{prf} For this proof, let $T\in \linmap (V)$ be finite-dimensional ($\dim V \neq \infty$).

  \Rightarrowdirection Suppose $T$ is diagonalizable. Thus there is a basis $v_1, \ddd, v_n$ of $V$ consisting of eigenvalues of $T$.  Let $\lambda_1, \ddd, \lambda_m$ be said distinct eigenvalues of $T$. % TODO: ?
  \begin{equation}
    \implies \forall v_j \; \exists \lambda_k $ \st $ (T-\lambda_k I)v_j = 0.
  \end{equation}

  Thus
  \begin{equation}
    (T-\lambda_1) \cdots (T-\lambda_m I)v_j =0,
  \end{equation}

  which implies the minimal polynomial equals $(z-\lambda_1)\cdots(z-\lambda_m).$

  \Leftarrowdirection Suppose the minimal polynomial of $T$ equals $(z-\lambda_1) \cdots (z-\lambda_m)$ for some $\lambda_1, \ddd, \lambda_m \in  \myF$ with $\lambda_i \neq \lambda_j$.
  $\implies$ $(T-\lambda_1 I) \cdots (T-\lambda_m I)=0.$ Hence, if we drop one factor we get
  \begin{equation}
    \label{eq: (t-l1 I)***(t-l_{m-1}I)=0}
    (T-\lambda_1 I) \cdots (T-\lambda_{m-1} I)=0.
  \end{equation}

  Suppose $m=1$. Then $T-\lambda_1 I = 0$ $\iff$ $T=\lambda_1 I$, so $T$ is diagonalizable. This is our base case for induction over $m \in \nat$.
  Now suppose $m>1$ and the desired result holds for all values smaller than $m$. Like in the proof of theorem \ref{thm: necessary and sufficient condition to have an upper-triangular-matrix}, we observe that $\myrange (T-\lambda_m I)$ is invariant under $T$ (using \ref{thm: null space and range of p(T) are invariant under T} with $p(z) := z-\lambda_m$). Now, let
  \begin{equation}
    \label{eq: 2nd definition of U}
    U := \myrange(T - \lambda_m I)
  \end{equation}

  This means that $\left . T \right |_{U)} $ is an operator on $U$.
  If $u \in \myrange (T-\lambda_m I)$, then $u = (T-\lambda_m I)v$ for some $v \in V$. Together with \eqref{eq: (t-l1 I)***(t-l_{m-1}I)=0}, we have that
  \begin{align}
    \label{eq: equation for u and v}
    (T-\lambda_1 I) \cdots (T- \lambda_{m-1} I)u
    &=(T-\lambda_1 I) \cdots (T-\lambda_m I)v \\
    &=0. % \mytext{(since $v$ consists of $v_j$'s)}
  \end{align}

  Since $u \in U$ was arbitrary, we have that $q(z) := (z-\lambda_1)\cdots (z-\lambda_{m-1})$ is a polynomial \st $q(T)(u)=0 \quad \forall u \in U$. By \ref{thm: every zero polynomial is a multiple of the minimal polynomial}, $q(z)$ is a polynomial multiple of the minimal polynomial of $\left . T \right |_{U}$.
  By our induction hypothesis, there is a basis of $U$ consisting of eigenvectors of $\left . T \right |_U$ and therefore, as well as eigenvectors of $T$.
  Suppose $x\in U \cap \mynull (T-\lambda_m I)$. Hence, $Tx = \lambda_m x$, because $Tx -\lambda Ix=0$. Equation \eqref{eq: equation for u and v} implies that
  \begin{equation}
    \begin{aligned}
      0 & = (T-\lambda_1 I) \cdots (T- \lambda_{m-1} I)x \\
      & = (\lambda_{m}I-\lambda_1 I) \cdots (\lambda_{m}I- \lambda_{m-1} I)x \\
      & = (\lambda_{m}-\lambda_1 ) \cdots (\lambda_{m}- \lambda_{m-1} )x \\
    \end{aligned}
  \end{equation}

  Because $\forall i\neq j: \lambda_j \neq \lambda_k$, this can only happen if $x=0$. Therefore,
  \begin{equation}
    U \cap \mynull (T-\lambda_m I) = \{0\}.
  \end{equation}

  Thus, $U + \mynull (T-\lambda_m I)$ is a direct sum by \ref{thm: sum and intersection of two subspaces}.
  \[
    U + \mynull (T-\lambda_m I) = U \oplus \mynull (T-\lambda_m I)
  \]

  Since $(T-\lambda_m I) \in \linmap (V)$ we can use the Rank Nullity Theorem \ref{rank-nullity-theorem} to conclude that
  \begin{equation}
    \dim V = \dim U + \dim \mynull (T-\lambda_m I).
  \end{equation}

  Theorem \ref{thm: a sum is a direct sum if and only if the dimensions add up} states that a sum is a direct sum $\iff$ the dimensions add up to the dimension of the sum. This means that we can rewrite the equation above to
  \begin{equation}
    \dim V = \dim (U \oplus \mynull (T-\lambda_m I))
  \end{equation}

  According to \ref{thm: subspace of full dimension equals the whole space}, together with the fact that $U \oplus \mynull (T-\lambda_m I) \subseteq V$, this implies
  \begin{equation}
    V=U \oplus \mynull (T-\lambda_m I)
  \end{equation}

  Every vector in $\mynull (T-\lambda_m I)$ is an eigenvector of $T$ with eigenvalue $\lambda_m$. Earlier in this proof we saw that there is basis of $\myrange(T-\lambda_mI)$ consisting of eigenvectors $T$ using our induction hypothesis on this smaller subspace of $V$. Adjoining to that basis a basis of $\mynull(T-\lambda_mI)$ gives a basis of $V$ consisting of eigenvecors of $T$. The matrix $\mmatrix(T)$ of $T$ with respect to this basis is a diagonal matrix, as desired, using \ref{thm: conditions equivalent to diagonalizability}. According to that theorem, $V$ has a basis consisting of eigenvectors of $T \in \linmap(V)$ $\iff$ it is diagonalizable. As a side node, this theorem states that $V = E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m, T)$, which correlates with $V=U\oplus \mynull (T-\lambda_m I)$.
\end{prf}

\setcounter{thm}{64}
%5.65
\begin{thm}[restriction of diagonalizable operator to invariant subspace]
  \label{thm: restriction of diagonalizable operator to invariant subspace}
  Let $T\in \linmap(V)$. Suppose $T$ is diagonalizable and $U$ is a subspace of $V$ that is invariant under $T$.
  \begin{equation}
    \implies$ $\left.T\right|_U$ is a diagonalizable operator on $U.
  \end{equation}
\end{thm}
\begin{prf}
  %  Diagonazability of $T$ $\mathsmaller{\overset{\text{\ref{thm: necessary and sufficient condition for diagonalizability}}}{\iff}}$ the minimal polynomial of $T$ equals
  Diagonazability of $T$ $\iff_{(\ref{thm: necessary and sufficient condition for diagonalizability})}$ the minimal polynomial of $T$ equals
  \begin{equation}
    (z-\lambda_1)\cdots(z-\lambda_m) \mytext{for} \lambda_j \neq  \lambda_k.
  \end{equation}

  By \ref{thm: minimal polynomial of a restriction operator}, the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $\left.T\right|_U$.
  Hence the minimal polynomial of $\left.T\right|_U$  has the form required by \ref{thm: necessary and sufficient condition for diagonalizability}, which shows that $\left.T\right|_U$ is diagonalizable. It consists of any factor of the form $(z-\lambda_k)$.
\end{prf}



% TODO: Gerhgorin Disk Theorem Def 5.66 and Theorem 5.67